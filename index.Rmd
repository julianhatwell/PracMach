```{r prologue, include=FALSE}
library(knitr)
library(stringr)
library(readr)
library(caret)
library(lattice)
library(ggplot2)
library(gridExtra)
library(parallel)
library(doParallel)

knitr::opts_chunk$set(warning = FALSE
                      , message = FALSE
                      , echo = FALSE
                      )

knitr::opts_template$set(
  fig.wide = list(fig.height = 4.5, fig.width = 8)
  , fig.wideX = list(fig.height = 3, fig.width = 8)
)

# the files are in my working directory
# please download them from here if you want to follow the code chunks

# training: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

# testing: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

training <- read_csv("pml-training.csv")
testing <- read_csv("pml-testing.csv")

# utility functions
getColClasse <- function(df) {
  which(names(df) == "classe")
}
# a nice colour palette
myPal <- c("#8DD3C7", "#B0A8B3", "#9FB5D6", "#9EC0FA", "#DB9082")
myPal.range <- colorRampPalette(c("#FFFFFF", c(myPal[3:1] )))
```

---
title: "Machine Learning Assignment"
author: "Julian Hatwell"
date: `r format(Sys.time(), "%b %Y") `
output: html_document
  html_document: 
    keep_md: yes
---

# Introduction

Data have been collected from accelerometers placed on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform dumbbell lifts correctly and incorrectly in 4 different ways, giving a total of 5 classifications (variable "classe" with possible values "A" to "E" in the dataset).  

More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

In this assigment, the objective is simply to use a training data partition to create a model that can correctly predict the classification for a small test set of 20 observations, derived from the same original data set.  

The data are already separated into training and testing sets. The training set will be split further. 80% will be used to train the prediction models and 20% held back to estimate the out of sample error rate.

```{r creating_training_validation_sets, echo=TRUE}
# FOR BREVITY I'VE HIDDEN MOST OF THE UTILITY CODE IN THIS REPORT. 
# YOU CAN CHECK THE RMD FILE IF INTERESTED.

# I've already loaded the data into R.
# This code will create two sets, split 80/20
set.seed(23)
training_ids <- createDataPartition(y = training$classe, p = 0.8, list = FALSE)

training_set <- training[training_ids,]
validation_set <- training[-training_ids,]
```

## Selecting & Creating Features

### Step one: Exploratory Data Analysis:

#### 1. Paper and background reading

The data set is structured in the form of a stream coming from the sensors worn by the participants. There are three date & time information variables which give time series information so the data can be analysed in time order.

The team who have collected the data have divided it into time windows as labelled in the num\_window variable and the variable new\_window (levels: "No", "Yes"), where value "Yes" indicates a time window boundary.

#### 2. Manual review of tabular data set
Using very general R functions such as head, tail, summary and looking at ranges of numeric variables, levels and frequency of factor variables etc, the following observations have been made:

* Around 50-60 variables are continuously populated without missing values. 
* Something in the region of 100 variables have been added to hold statistics for each time window.
* These summary variables only hold data for rows where new_window = "Yes" (a tiny fraction)

None of these additional summary variables are useful for the assignment and will be removed.

* The timestamps and the num_window variable appear in the test set as well as the training set and presumably will predict the classe with 100% accuracy due to the chronological relationship to the training set.

```{r time_series_plot, opts.label='fig.wideX'}
xyplot(factor(training_set$classe)~training_set$num_window
      , col=myPal[factor(training_set$classe)]
      , pch ="|", cex = 3
      , main = list(
        label = "Classifications appear in strict time order\nand will be too easy to predict"
                    , cex = 0.8)
      , xlab = "Time Series Window (num_window)"
      , ylab = "classe (classification)")
```

In the spirit of the intention behind this assignment (i.e. to simulate a real world machine learning scenario), these variables are also removed from the training set.

```{r remove_unnecessary_columns, echo=TRUE}
# I'll write this as a function that "cleans" a data frame of all the unwanted columns.
# This is so I can easily re-use the function on validation and test sets.
rmUnwantedCols <- function(df) {
  # determine which columns are sparsely populated
  keepCols <- apply(df, 2, function(j) {sum(is.na(j) | j == "") == 0})
  # add indices of other columns to drop (time, window and participant name)
  keepCols[1:7] <- FALSE
  return(df[,keepCols])
}

training_set <- rmUnwantedCols(training_set)
# number of predictors now, reduced from 159
dim(training_set)[2] - 1
```

#### 3. Perform some exploratory plots

Presented here is the summary of the visual exploratory data analysis findings:

```{r plot_setup}
f_classe <- factor(training_set$classe)
myScatterPlot <- function(j, df) {
  xyplot(df[[j]]~I(1:nrow(df))
                , groups = f_classe 
                , col = myPal
                , ylab = j
                , xlab = "index"
  )
}

myViolinPlot <- function(j, df) {
  bwplot(df[[j]]~f_classe
                , groups = f_classe
                , col = myPal
                , scales = list(y = list(tck = c(1, 0)))
                , panel = panel.superpose
                , panel.groups = panel.violin
                , ylab = j)
}

layoutPlots_4 <- function(vars, plotFunc, df) {
  print(plotFunc(vars[1], df), pos = c(0,0.5, 0.5, 1), more = TRUE)
  print(plotFunc(vars[2], df), pos = c(0.5, 0.5, 1, 1), more = TRUE)
  print(plotFunc(vars[3], df), pos = c(0, 0, 0.5, 0.5), more = TRUE)
  print(plotFunc(vars[4], df), pos = c(0.5, 0, 1, 0.5))
}
```

```{r common_plotting_params}
df <- training_set
vars <- names(df)[-getColClasse(df)]
```
```{r exploratory_scats_full, eval=FALSE}
# This code chunk should be silent in the final version
for (var in vars) {
    s <- myScatterPlot(var, df)
    print(s)
}
```
```{r exploratory_violins_full, eval=FALSE}
# This code chunk should be silent in the final version
for (var in vars) {
    v <- myViolinPlot(var, df)
    print(v)
}
```

Violin plots have been selected for this investigation as they are excellent for visualising variation between groups. 

Alternative plot forms include:

  * boxplots with points overlaid
    + these were found to be too busy given the size of the data set
  * density plots
    + again the visual clarity breaks down for more than 2-3 groups and there are 5 classifications in this data set 

Below are just a few examples from the 50 or so possible predictors. 

```{r exploratory_plots_examples, opts.label='fig.wide'}
# Given the large number of potential predictors, there are a lot of plots of interest. 
# For brevity only a couple of interesting examples are included. 
# Code to setup the plot function and produce many more plots is in the Rmd file.
exampleVars <- c("gyros_arm_x", "magnet_belt_y", "accel_forearm_x", "yaw_forearm")

layoutPlots_4(exampleVars, myViolinPlot, training_set)
```

There are significant numbers of predictors that separate classe "A"  very well, e.g. "`r exampleVars[1]`" variable.

Also many variables could be used to separate classe "E", e.g. "`r exampleVars[2]`"

Classes "B", "C" and "D" have much more subtle difference on the whole. e.g. "`r exampleVars[3]`"

And quite a number where there don't appear to be clear differences in the distributions, such as that shown in "`r exampleVars[4]`"

### Step 2: Preprocessing & Dimension Reduction

#### 1. Deal with problematic data
```{r outlier_plot_setup, opts.label='fig.wide'}
colClasse <- getColClasse(training_set)
outlier <- which.min(training_set$gyros_dumbbell_x)
outVars <- c("gyros_dumbbell_y", "gyros_dumbbell_x", "accel_dumbbell_z", "gyros_arm_y")
myOutlierPlot <- function(j, df) {
  xyplot(df[[j]]~I(1:nrow(df))
                , groups = f_classe
                , panel = function(x, y, col, ...) {
                  panel.xyplot(x, y, col = myPal, ...)
                  panel.points(outlier, y[outlier]
                               , col = "black"
                               , pch = 19)
                }
                , ylab = j
                , xlab = "index")
}
```

There appears to be one problematic observation that has the following characteristics:

* It is a highly significant outlier in at least 5 of the predictors e.g. "`r outVars[1]`" & "`r outVars[3]`"
* It is a member of an outlying cluster in a single predictor, "`r outVars[3]`"
* It is unremarkable in the remaining predictors e.g. "`r outVars[4]`"

This point may be a knock or a jolt or it may be a malfunction as it appears only the readings from the dumbbell sensor. It is obviously not part of the routine exercise, although there is no way to validate this.

To correct this value, K Nearest Neighbours Imputation will be used, based on index position. This is appropriate given that the data originates as a time series and the nearest neighbours are by time order.

This operation is expected to bring its value into the normal range where it is a solo outlier whilst leaving it in place where it is part of a cluster or where it is already normal.

```{r outlier_removal, echo=TRUE, opts.label='fig.wide'}
layoutPlots_4(outVars, myOutlierPlot, training_set)

training_set[outlier, -colClasse] <- NA
KNN <- 6 
KNN_range <- (outlier-floor(KNN/2)):(outlier+floor(KNN/2))
KNN_imputation <- apply(training_set[KNN_range,-colClasse]
                        , 2
                        , mean, na.rm = TRUE
                        )
training_set[outlier, -colClasse] <- KNN_imputation

# replot the same 4 examples to see if it's worked - and it has
layoutPlots_4(outVars, myOutlierPlot, training_set)
```

#### 2. Extract Features

Creating good features depends to some extent on which prediction algorithm will be used. Model based approaches are sensitive to skewed or irregular distribution, which have been found in this data set.

Other approaches that are less sensitive, such as random forests are very computationally intensive and will be slow to run for large numbers of predictors and large data sets.

To improve model training performance in both areas (accuracy and time to train), an attempt is  made to transform the predictors and reduce their  number. 

The method used for this is Principle Components Analysis (PCA) which captures most of the variation between the predictors into a smaller number of dimensions.

```{r processing_setup, echo=TRUE}
colClasse <- getColClasse(training_set)
predictors <- training_set[,-colClasse]
# Capture these common values to use on training, validation and final test sets
minValues <- apply(predictors, 2, min)
stDevs <- apply(predictors, 2, sd)

# This function will shift each variable by addition. 
# Instead of centering the means around zero,
# the intention is to ensure that each variable
# has its entire range greater than zero so I can then take the natural log.
# Adding a little extra room (6 standard devs) for extreme outliers 
# to ensure that I can process the validation and training sets with the same parameters
myTransform <- function(df) {
  for (j in names(predictors))
    df[[j]] <- log(df[[j]] + 
      abs(minValues[j]) + 
      6 * stDevs[j])
  return(df)
}
```

```{r pca_processing, echo=TRUE}
myPreProc <- preProcess(
              myTransform(training_set)[,-colClasse]
              , method = "pca"
              , thresh = 0.8)
training_pca <- predict(myPreProc, myTransform(training_set))
# The number of predictor dimensions has been reduced to:                      
dim(training_pca)[2] -1
```

```{r common_plotting_params_pca, eval=FALSE}
# Interesting to see what these look like
df <- training_pca
vars <- names(df)[-getColClasse(df)]
```
```{r exploratory_scats_full, eval=FALSE}
```
```{r exploratory_violins_full, eval=FALSE}
```

## Selecting an algorithm
The following steps were used to try to find a good model:

1. Review PML lectures, internet search and book search (e.g. "Introduction to Statistical Learning with R").

2. Pick algorithms and functions that are considered useful for a classification problem, and possibly this specific problem.

3. Train selected algorithms on both the full set of predictors  and PCA transformed training data.

The algorithms chosen are:  

  * Quadratic Linear Discriminant Analysis (qda)
    + This classifier is inherently multiclass, with no tuning parameters.
  * Stochastic Gradient Boosting (gbm)
    + It is hoped that this classifier can combine a large number of weak predictors and separate especially the more similar classes "B", "C" and "D".
    + It should do better with the full set of predictors, prior to PCA transformation.
  * Random Forest (rf)
    + This algorithm highly regarded for accuracy but takes a long time to build.
  
## Building Models with Cross Validation

Thanks to the caret package, it is not necessary manually to code loops for k-fold cross validation in model building. The cross validation will be performed by the train function, using an appropriate trainControl setting.

```{r common_training_params, echo=TRUE}
# This trainControl object will be passed into the train function calls.
# 5 fold cross validation will be performed on each model as it is being trained. 
# 5 has been chosen because there are 4 cores on my processor. 4 will train while one is held back
tc <- trainControl(method = "cv", number = 5, allowParallel = TRUE)
```

```{r get_or_train, echo=TRUE}
# This function will load each model from a file if one exists.
# Otherwise it will train a new model and save it.
# I don't want to use the knitr cache for this because
# I also want to use the objects outside of the knitr environment and they take so long to build.
get_or_train <- function(algo, trans) {
  
  modelName <- paste(algo, trans, sep = "_")
  modelFileName <- paste0("model_", modelName, ".RData")
  
  if (file.exists(modelFileName)) {
    attach(modelFileName, warn.conflicts = FALSE)
    } else {
      dsName <- paste0("training_", trans)
      
      # set up parallel processing
      p_clus <- makeCluster(detectCores())
      registerDoParallel(p_clus)
      
      # build the model
      assign(modelName, train(classe~., data = get(dsName),  trControl = tc, method = algo))
      
      # close parallel processing
      stopCluster(p_clus)
      
      # naive cache
      # save out to an external file for re-use
      model <- get(modelName)
      save(model, file = modelFileName)
    }
  return(model)
  }
```

```{r model_setup}
# a matrix of names to use in for loops
algorithms <- c("gbm", "qda", "rf")
transforms <- c("pca", "set")
models <- cbind(algo = rep(algorithms, each = 2),trans = transforms)
models <- cbind(models
                , model = paste(models[,1]
                        , models[,2]
                        ,"model", sep = "_")
                )
models <- cbind(models
                , result = gsub("model", "cm", models[,3])
                , pred = gsub("model", "pred", models[,3])
                , mark = gsub("model", "mark", models[,3])
                )
```

```{r model_building, echo=TRUE}
# Algorithms qda, gbm and rf over both full set and pca transformed.
models[, c("model", "algo", "trans")]
n <- nrow(models)
for (m in 1:n) {
  assign(models[m,"model"]
         , get_or_train(algo = models[m,"algo"]
                        , trans = models[m, "trans"])
  )
}
```

## Results & Evaluation

Thanks to the caret package, the cross validation has been run during the model training and provides a good estimate of the likely out of sample error rate. This is returned in the \$results\$accuracy list item on the model object and can also be seen by printing the model summaries. For convenience these have been collected, tabulated (as cvAccuracy) and plotted below.

A better estimate of out of sample error rate is evaluated with the validation set which was held back at the beginning of the assignment.  

First, it has to be run through the same transformations:

```{r transform_validation_set, echo=TRUE}
# Enact the EXACT same data cleaning and transformation steps.
# Remove the unwanted columns
validation_set <- rmUnwantedCols(validation_set)

# Using parameters from the training set
# namely, the min values, st devs and the PCA object
# Shift above zero, take the log and perform PCA on the validation set
validation_pca <- predict(myPreProc, myTransform(validation_set))
```

```{r confusion_matrices}
for (m in 1:n) {
  assign(models[m,"result"]
         , confusionMatrix(predict(get(models[m, "model"])
                                        , get(paste0("validation_", models[m,"trans"])))
                                , validation_set$classe
                                )
         )
}
```  

Confusion matrices are produced using standard caret package functions (not shown, see Rmd file if required). The results are tabulated here for convenience:

```{r evaluation_stats}
buildTimes <- numeric(n)
cvAccuracy <- numeric(n)
accuracy <- matrix(0, n, 3, dimnames = list(NULL, c("Accuracy", "Accuracy_Lower", "Accuracy_Upper")))

for (m in 1:n) {
  buildTimes[m] <- get(models[m, "model"])$times$everything[3]
  cvAccuracy[m] <- round(max(get(models[m, "model"])$results$Accuracy),4)
  accuracy[m,] <- round(get(models[m, "result"])$overall[c(1, 3:4)], 4)
}

modelStats <- data.frame(model = models[, "model"]
                         , buildTimes = buildTimes
                         , cvAccuracy = cvAccuracy
                         , accuracy
                         , inSample_Error_Rate = 1 - cvAccuracy
                         , outOfSample_Error_Rate = 1 - accuracy[,1])

# a little utility function for the text
row.names(accuracy) <- models[,"model"]
accuracyConfInt <- function(m) { accuracy[m, 2:3] }

# Tablulate the results
kable(modelStats)
```

From the table it is clear that the cross validation has provided an estimate of out of sample error rate which is incredibly close to the estimate produced through prediction against the validations set.  

```{r evaluation_plots, opts.label='fig.wide'}
xyplot(Accuracy~buildTimes
       , data = modelStats
       , panel = function(x,y) {
         panel.loess(x, y
              , span = 1.5
              , degree = 1
              , lwd = 10
              , col = myPal[1]
              , alpha = 0.3)
         panel.xyplot(x, y
              , pch = 19
              , col = myPal[5]
              , cex = (1 + cvAccuracy) * cvAccuracy)
         panel.segments(x + c(rep(5,4), rep(-5,2)), y
                        , (x + c(rep(60,4), rep(-60,2))/3), y)
         panel.text(x + c(rep(90,4), rep(-80,2))
                    , y
                    , modelStats$model)
       }
       , main = list(columns = 2, label = "Model Performance and Training Times"
                   , cex = 0.8)
       , xlab = "Training Time (seconds)"
       , ylab = "Out of sample accuracy\nEstimated using validation set"
       , key = list(title = "Accuracy estimate from Cross Validation\n performed by the train function"
                    , cex = 0.5
                    , columns = 2
                    , text = list(as.character(range(cvAccuracy)), cex = 0.75)
                , points = list(pch = 19, col = myPal[5], cex = range((1 + cvAccuracy) * cvAccuracy))
                )
)
```

The graph shows how model accuracy increases with model complexity in a non-linear way. More and more computation cycles are required for diminishing returns. This is an important consideration for exploratory and non-critical applications with large data sets.

```{r confusion_matrices_plots, opts.label='fig.wide'}
confmats <- data.frame()
for (m in 1:n) {
  confmats <- rbind(confmats
                    , cbind.data.frame(model =  models[m, "result"]
                                       ,  get(models[m, "result"])$table)
  )
}

levelplot(sqrt(Freq)~Prediction+Reference | model
          , data = confmats
          , shrink = c(0.25, 1)
          , col.regions = myPal.range
          , strip = strip.custom(bg = myPal[4]
                                 , par.strip.text = list(cex = 0.8))
          , scales = list(x = list(alternating = c(1,0,1)
                                   , tck = 1:0)
                          , y = list(alternating = c(0,3))
                          )
          , between = list(x = 0.2, y = 0.2)
          , par.settings = list(axis.line = list(col = myPal[2]),
                                strip.border = list(col = myPal[2]))
          , main = list(label = "Levelplots of the confusion matrices"
          , cex = 0.8)
          , sub = list(label = expression(paste("Colour and size scaled for emphasis to ", sqrt("Frequency")))
                       , cex = 0.75)
)
```

The random forest model on the full set of predictor variables is the most accurate. However, took over `r round(modelStats[modelStats$model == "rf_set_model", "buildTimes"]/60,0)` minutes to train. The process was parallelised over `r detectCores()` cores.

The out of sample error rate for this model is predicted to be between `r 1 - accuracyConfInt("rf_set_model")[2]` and `r 1 -  accuracyConfInt("rf_set_model")[1]`   

Reducing the number of predictor variables with PCA increased the error rate 10 fold (though it was initially very small) to between `r 1 - accuracyConfInt("rf_pca_model")[2]` and `r 1 -  accuracyConfInt("rf_pca_model")[1]`, but cut the build time to under `r ceiling(modelStats[modelStats$model == "rf_pca_model", "buildTimes"]/60)` minutes. 

As expected, Stochastic Gradient Boosting performed very will with between `r 1 - accuracyConfInt("gbm_set_model")[2]` and `r 1 - accuracyConfInt("gbm_set_model")[1]` accuracy when the full set of predictors were used but was much less useful on the PCA transformed set.

Quadratic Linear Discrimiation was the poorest performer overall, with by far the lowest accuracy when using the PCA transformed set. However, it took a fraction the time to build and could be useful for exploratory and non-critical applications especially where very large data sets are involved.

For the purposes of this assignment, the most accurate model, rf\_set\_model, will be used for the final test submission. In a test of 20 samples, the chances of getting a single item incorrect are between `r 20 * (1 - accuracyConfInt("rf_set_model"))[2]` and `r 20 * (1 - accuracyConfInt("rf_set_model"))[1]`.

### Outcome

The test results are assumed to be deducible using the num_window variable in the original data set as described in the first part of this report. 

Based on this assumption, it is possible to comment on the final model performance.

```{r generate_predictions}
# Tests
# Enact the EXACT same data cleaning and transformation steps.
# Remove the unwanted columns
testing_set <- cbind(rmUnwantedCols(testing)[, -53], classe = character(20))

# Shift above zero, take the log and perform PCA
# Using only parameters built from the training set
testing_pca <- predict(myPreProc
                          , myTransform(testing_set))

for (m in 1:n) {
  trans <- ifelse(grepl("pca", models[m,"pred"]), "pca", "set")
  assign(models[m,"pred"], predict(get(models[m,"model"])
                           , get(paste0("testing_", trans)))
  )
}

answers <- data.frame()
for (m in 1:n) {
  answers <- rbind(answers, data.frame(id = 1:20
                                , model = models[m,"model"]
                                , prediction = get(models[m, "pred"])
                                , reference = sapply(testing$num_window
                                    , function(nw) { 
                                      unique(training[training$num_window == nw
                                          , "classe"])
                                      }
                                    )
                                )
                  )
}
```

```{r_predictions_plots, opts.label='fig.wide'}
g <- ggplot(data = answers, aes(x = id)) +
  geom_point(aes(y = reference)
             , colour = myPal[1], size = 7, alpha = 0.5) +
  geom_point(aes(y = prediction)
             , colour = myPal[5], size = 3, shape = 15) +
  facet_wrap(~ model, ncol = 2) + theme_bw() +
  theme(strip.background = element_rect(fill=myPal[4])) + 
  labs(list(x = "Problem Id", y = "Prediction"))

g
```

*Red mark on green target denotes correct answer predicted. Incorrect answer predicted if mark misses green target*  

Both rf models have guessed all the answers correctly.  

For the models that used the full set of predictors, gbm and (somewhat surprisingly) qda have predicted well with only a single error. The same point was misclassified so perhaps this is an ambiguous case. Certainly it is useful to know that these much simpler models can do very well, given that qda\_set\_model took only 11 seconds to train compared to the random forest 11 minutes on the same data.  

The lower performance when using the PCA transformed data set was perhaps expected. The trade off between data compression and model accuracy is clear from this demonstration.